# Deep Learning Framework
torch>=2.0.0
transformers>=4.35.0

# LoRA and Parameter-Efficient Fine-Tuning
peft>=0.11.0
bitsandbytes>=0.43.0
accelerate>=0.27.0

# Data Processing
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# Tokenization for BERTweet
emoji>=2.8.0

# Progress bar
tqdm>=4.66.0

# Logging and Experiment Tracking
tensorboard>=2.14.0

# Optional: For faster tokenization
tokenizers>=0.14.0

# Data Augmentation (for Phase 2)
# nlpaug>=1.1.11  # Uncomment when implementing data augmentation

